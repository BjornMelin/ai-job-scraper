# vLLM Production Configuration - AI Job Scraper
# Based on ADR-010 specifications for Qwen3-4B-Instruct-2507-FP8

model:
  name: "Qwen/Qwen3-4B-Instruct-2507-FP8"
  quantization: "fp8"
  kv_cache_dtype: "fp8"
  max_model_len: 8192
  trust_remote_code: true
  served_model_name: "Qwen3-4B-Instruct-2507-FP8"

# Performance optimization settings
performance:
  gpu_memory_utilization: 0.9
  swap_space: 4  # 4GB CPU swap space
  enable_prefix_caching: true
  max_num_seqs: 128
  disable_log_requests: true
  max_num_batched_tokens: 2048
  enable_chunked_prefill: true
  scheduler_delay_factor: 0.0

# Server configuration
server:
  host: "0.0.0.0"
  port: 8000
  api_key: "${VLLM_API_KEY}"
  enable_cors: true
  
# Monitoring and metrics
monitoring:
  enable_metrics: true
  metrics_port: 8001
  log_level: "INFO"
  
# Structured output defaults
structured_output:
  default_temperature: 0.1
  max_tokens: 2000
  guaranteed_json: true

# System requirements validation
requirements:
  vllm_version: ">=0.6.2"
  cuda_version: ">=12.1"
  python_version: ">=3.9"
  hardware: "NVIDIA GPU with Compute Capability 8.9+ (RTX 4090)"
  
# OpenAI API compatibility settings
openai_api:
  chat_completions: true
  embeddings: false
  models_endpoint: true
  compatibility_mode: true