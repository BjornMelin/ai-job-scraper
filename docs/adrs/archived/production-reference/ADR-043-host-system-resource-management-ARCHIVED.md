# ADR-043: Host System Resource Management

## Title

Host System Resource Management for vLLM swap_space Operations and PCIe Bandwidth Optimization

## Version/Date

1.0 / August 19, 2025

## Status

**Accepted** - Critical for vLLM two-tier deployment (ADR-042)

## Description

Establish comprehensive host system resource management strategies for optimal vLLM two-tier deployment performance. This architecture manages RAM allocation for swap_space operations, monitors PCIe bandwidth utilization, implements container resource limits, and provides alerting for resource exhaustion prevention. Essential for achieving 2-3x performance improvement validated through expert research.

## Context

### Critical Research Findings

**Host System Requirements for vLLM swap_space:**

- **Host RAM allocation**: swap_space × num_pools + 20% overhead (18GB+ required)
- **PCIe bandwidth monitoring**: 10GB/s sustained threshold for alerts
- **Container limits**: Resource reservations with overflow buffers
- **Performance monitoring**: Latency regression detection (P95/P99 ≤15% increase)

### Integration with vLLM Two-Tier Architecture

**ADR-042 Integration Requirements:**

- Support throughput pool (swap_space=2GB) + long-context pool (swap_space=16GB)
- Monitor host system impact of H2D memory transfers during swap operations
- Prevent resource exhaustion that could impact vLLM performance
- Provide early warning system for capacity planning and scaling

### Hardware Optimization Context

**RTX 4090 Hardware Configuration:**

- **GPU Memory**: 24GB VRAM with 1,008 GB/s memory bandwidth
- **Host Integration**: PCIe 4.0 x16 connection for swap_space operations
- **System RAM**: 64GB recommended for optimal swap_space allocation
- **Performance Target**: 85-90% GPU utilization with swap overflow capability

## Related Requirements

### Functional Requirements

- **FR-HOST-01**: Monitor host RAM usage with swap_space allocation tracking
- **FR-HOST-02**: Track PCIe bandwidth utilization with automated alerting
- **FR-HOST-03**: Implement container resource limits with overflow buffers
- **FR-HOST-04**: Provide comprehensive resource metrics for capacity planning

### Non-Functional Requirements

- **NFR-HOST-01**: Host RAM monitoring with 90% utilization alerts
- **NFR-HOST-02**: PCIe bandwidth monitoring with 10GB/s alert threshold  
- **NFR-HOST-03**: Container memory limits: 20GB (18GB swap_space + 2GB overhead)
- **NFR-HOST-04**: Resource monitoring overhead <1% CPU utilization

### Performance Requirements

- **PR-HOST-01**: Monitor swap_space allocation: 2GB + 16GB + 2GB overhead = 20GB total
- **PR-HOST-02**: PCIe H2D transfer monitoring with sustained bandwidth tracking
- **PR-HOST-03**: Memory leak detection with automatic cleanup triggers
- **PR-HOST-04**: Resource usage trend analysis for capacity planning

## Related Decisions

- **Critical for ADR-042**: vLLM Two-Tier Deployment Strategy (provides resource management)
- **Supports ADR-044**: Production Monitoring and Alerting Strategy (resource metrics)
- **Enables ADR-035**: Final Production Architecture (host system optimization)
- **Coordinates with ADR-041**: Performance Optimization Strategy (resource efficiency)

## Decision

**Deploy Comprehensive Host Resource Management** with these core components:

### 1. Host RAM Allocation and Monitoring

**Expert-Validated Resource Allocation Strategy:**

```python
import psutil
import asyncio
import logging
from prometheus_client import Gauge, Counter, Histogram
from dataclasses import dataclass
from typing import Dict, List, Optional

@dataclass 
class HostResourceConfig:
    """Host system resource configuration for vLLM deployment."""
    
    # vLLM swap_space allocations (from ADR-042)
    throughput_pool_swap_gb: int = 2      # 2GB for throughput pool
    longcontext_pool_swap_gb: int = 16    # 16GB for long-context pool
    overhead_buffer_gb: int = 2           # 2GB overhead buffer
    
    # Alert thresholds
    ram_warning_threshold: float = 0.9    # 90% RAM utilization
    ram_critical_threshold: float = 0.95  # 95% RAM utilization
    pcie_bandwidth_threshold: float = 10.0 # 10 GB/s sustained
    
    # Container limits
    container_memory_limit_gb: int = 24   # 20GB swap + 4GB app overhead
    container_memory_reservation_gb: int = 20  # Guaranteed allocation

class HostRAMMonitor:
    """Comprehensive host RAM monitoring for vLLM swap_space operations."""
    
    def __init__(self, config: HostResourceConfig):
        self.config = config
        
        # Prometheus metrics
        self.host_ram_total = Gauge('host_ram_total_bytes', 'Total host RAM')
        self.host_ram_used = Gauge('host_ram_used_bytes', 'Used host RAM') 
        self.host_ram_available = Gauge('host_ram_available_bytes', 'Available host RAM')
        self.host_ram_utilization = Gauge('host_ram_utilization_percent', 'RAM utilization percentage')
        
        # vLLM-specific metrics
        self.vllm_swap_allocated = Gauge('vllm_swap_space_allocated_bytes', 'Allocated swap space')
        self.vllm_swap_used = Gauge('vllm_swap_space_used_bytes', 'Used swap space')
        
        # Alert counters
        self.ram_warnings = Counter('host_ram_warnings_total', 'RAM usage warnings')
        self.ram_critical = Counter('host_ram_critical_total', 'RAM critical alerts')
        
    async def monitor_ram_usage(self):
        """Continuous RAM monitoring with vLLM swap_space awareness."""
        
        while True:
            try:
                # Get current RAM statistics
                ram_stats = psutil.virtual_memory()
                
                # Update basic metrics
                self.host_ram_total.set(ram_stats.total)
                self.host_ram_used.set(ram_stats.used)
                self.host_ram_available.set(ram_stats.available)
                self.host_ram_utilization.set(ram_stats.percent)
                
                # Calculate vLLM swap space allocations
                allocated_swap = (
                    (self.config.throughput_pool_swap_gb + 
                     self.config.longcontext_pool_swap_gb + 
                     self.config.overhead_buffer_gb) * 1024 * 1024 * 1024
                )
                self.vllm_swap_allocated.set(allocated_swap)
                
                # Estimate current swap usage (requires vLLM integration)
                swap_usage = await self._estimate_vllm_swap_usage()
                self.vllm_swap_used.set(swap_usage)
                
                # Check alert thresholds
                utilization = ram_stats.percent / 100.0
                
                if utilization >= self.config.ram_critical_threshold:
                    await self._trigger_critical_ram_alert(utilization, ram_stats)
                    self.ram_critical.inc()
                    
                elif utilization >= self.config.ram_warning_threshold:
                    await self._trigger_ram_warning(utilization, ram_stats)
                    self.ram_warnings.inc()
                
                # Log resource status
                logging.info(f"Host RAM: {utilization:.1%} used "
                           f"({ram_stats.used // 1024 // 1024 // 1024}GB / "
                           f"{ram_stats.total // 1024 // 1024 // 1024}GB), "
                           f"vLLM swap allocated: {allocated_swap // 1024 // 1024 // 1024}GB")
                
            except Exception as e:
                logging.error(f"RAM monitoring failed: {e}")
            
            await asyncio.sleep(5)  # Monitor every 5 seconds
    
    async def _estimate_vllm_swap_usage(self) -> int:
        """Estimate current vLLM swap space usage."""
        try:
            # Integration with vLLM monitoring APIs
            # This would require vLLM-specific monitoring integration
            # For now, return estimated based on system memory pressure
            ram_stats = psutil.virtual_memory()
            
            # Rough estimate: if RAM is under pressure, swap is likely being used
            if ram_stats.percent > 85:
                # Estimate 50-80% of allocated swap is in use during high RAM usage
                allocated_swap = (
                    (self.config.throughput_pool_swap_gb + 
                     self.config.longcontext_pool_swap_gb) * 1024 * 1024 * 1024
                )
                return int(allocated_swap * 0.65)  # 65% estimated usage
            else:
                return 0  # Minimal swap usage when RAM is not under pressure
                
        except Exception as e:
            logging.warning(f"Failed to estimate vLLM swap usage: {e}")
            return 0
    
    async def _trigger_ram_warning(self, utilization: float, ram_stats):
        """Trigger RAM usage warning alert."""
        message = (
            f"Host RAM utilization high: {utilization:.1%} "
            f"(threshold: {self.config.ram_warning_threshold:.1%}). "
            f"Available: {ram_stats.available // 1024 // 1024 // 1024}GB. "
            f"vLLM swap_space operations may be impacted."
        )
        
        logging.warning(message)
        await self._send_alert("RAM Usage Warning", message, severity="warning")
    
    async def _trigger_critical_ram_alert(self, utilization: float, ram_stats):
        """Trigger critical RAM usage alert."""
        message = (
            f"CRITICAL: Host RAM utilization: {utilization:.1%} "
            f"(threshold: {self.config.ram_critical_threshold:.1%}). "
            f"Available: {ram_stats.available // 1024 // 1024 // 1024}GB. "
            f"vLLM performance degradation likely. Immediate action required."
        )
        
        logging.critical(message)
        await self._send_alert("CRITICAL RAM Usage", message, severity="critical")
```

### 2. PCIe Bandwidth Monitoring

**PCIe H2D Transfer Monitoring for swap_space Operations:**

```python
class PCIeBandwidthMonitor:
    """Monitor PCIe bandwidth for vLLM swap_space H2D transfers."""
    
    def __init__(self, config: HostResourceConfig):
        self.config = config
        
        # PCIe bandwidth metrics
        self.pcie_bandwidth_used = Gauge('pcie_bandwidth_used_gbps', 'PCIe bandwidth utilization')
        self.pcie_h2d_transfers = Counter('pcie_h2d_transfers_total', 'Host-to-device transfers')
        self.pcie_d2h_transfers = Counter('pcie_d2h_transfers_total', 'Device-to-host transfers')
        
        # Bandwidth saturation alerts
        self.pcie_saturation_events = Counter('pcie_saturation_events_total', 'PCIe saturation events')
        
        # Transfer latency tracking
        self.pcie_transfer_latency = Histogram('pcie_transfer_latency_seconds', 'PCIe transfer latency')
    
    async def monitor_pcie_bandwidth(self):
        """Monitor PCIe bandwidth utilization and swap_space transfer patterns."""
        
        while True:
            try:
                # Get PCIe bandwidth utilization
                bandwidth_usage = await self._get_pcie_bandwidth_usage()
                self.pcie_bandwidth_used.set(bandwidth_usage)
                
                # Track transfer patterns
                h2d_transfers, d2h_transfers = await self._get_transfer_counts()
                
                # Check for saturation conditions
                if bandwidth_usage > self.config.pcie_bandwidth_threshold:
                    await self._handle_pcie_saturation(bandwidth_usage)
                    self.pcie_saturation_events.inc()
                
                # Log bandwidth status
                logging.debug(f"PCIe bandwidth: {bandwidth_usage:.1f} GB/s "
                            f"(threshold: {self.config.pcie_bandwidth_threshold} GB/s)")
                
            except Exception as e:
                logging.error(f"PCIe monitoring failed: {e}")
            
            await asyncio.sleep(2)  # Higher frequency monitoring for bandwidth
    
    async def _get_pcie_bandwidth_usage(self) -> float:
        """Get current PCIe bandwidth utilization."""
        try:
            # Using nvidia-ml-py for GPU monitoring
            import pynvml
            
            pynvml.nvmlInit()
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # First GPU
            
            # Get memory transfer statistics
            mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
            util_info = pynvml.nvmlDeviceGetUtilizationRates(handle)
            
            # Estimate PCIe bandwidth usage based on memory utilization
            # RTX 4090 has ~1TB/s memory bandwidth, PCIe 4.0 x16 has ~64GB/s
            # During swap operations, PCIe becomes the bottleneck
            
            # Rough calculation: high memory utilization with swap_space indicates PCIe usage
            memory_utilization = mem_info.used / mem_info.total
            
            if memory_utilization > 0.85:  # High VRAM usage may trigger swap
                # Estimate PCIe usage based on memory pressure and utilization
                estimated_pcie_gbps = min(
                    util_info.memory * 0.64,  # Scale memory util to PCIe bandwidth
                    64.0  # PCIe 4.0 x16 theoretical maximum
                )
                return estimated_pcie_gbps
            else:
                return util_info.memory * 0.1  # Minimal PCIe usage during normal operation
                
        except ImportError:
            logging.warning("pynvml not available, using mock PCIe monitoring")
            return 0.0
        except Exception as e:
            logging.error(f"PCIe bandwidth monitoring failed: {e}")
            return 0.0
    
    async def _get_transfer_counts(self) -> tuple[int, int]:
        """Get H2D and D2H transfer counts."""
        # This would integrate with system-level monitoring
        # For now, return estimated values based on vLLM operations
        return 0, 0
    
    async def _handle_pcie_saturation(self, bandwidth_usage: float):
        """Handle PCIe bandwidth saturation event."""
        message = (
            f"PCIe bandwidth saturation detected: {bandwidth_usage:.1f} GB/s "
            f"(threshold: {self.config.pcie_bandwidth_threshold} GB/s). "
            f"vLLM swap_space transfers may be degraded. "
            f"Consider reducing swap_space allocation or scaling resources."
        )
        
        logging.warning(message)
        await self._send_alert("PCIe Bandwidth Saturation", message, severity="warning")
```

### 3. Container Resource Management

**Docker Resource Limits and Reservations:**

```yaml
# production-container-config.yaml
version: '3.8'

services:
  ai-job-scraper:
    image: ai-job-scraper:latest
    container_name: ai-job-scraper-production
    
    # Resource limits and reservations for vLLM swap_space
    deploy:
      resources:
        # Guaranteed resource allocation
        reservations:
          memory: 20G              # 18GB swap_space + 2GB overhead
          cpus: '4.0'              # 4 CPU cores reserved
          devices:
            - driver: nvidia
              count: 1             # Single RTX 4090
              capabilities: [gpu]
        
        # Resource limits (with overflow capacity)
        limits:
          memory: 24G              # 4GB overflow buffer for bursts
          cpus: '6.0'              # 2 additional CPUs for bursts
    
    # Environment configuration
    environment:
      # Host resource monitoring
      - HOST_RAM_WARNING_THRESHOLD=0.90
      - HOST_RAM_CRITICAL_THRESHOLD=0.95
      - PCIE_BANDWIDTH_THRESHOLD=10.0
      
      # vLLM swap_space configuration
      - VLLM_THROUGHPUT_SWAP_GB=2
      - VLLM_LONGCONTEXT_SWAP_GB=16
      - VLLM_OVERHEAD_BUFFER_GB=2
      
      # Container limits
      - CONTAINER_MEMORY_LIMIT_GB=24
      - CONTAINER_MEMORY_RESERVATION_GB=20
    
    # Volume mounts
    volumes:
      - ./models:/models:ro         # Model storage (read-only)
      - ./data:/data               # Application data
      - ./logs:/app/logs           # Log storage
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Docker API access
    
    # Health checks
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    
    # Resource monitoring
    sysctls:
      - net.core.somaxconn=65535   # Network optimization
    
    ulimits:
      memlock: -1                  # Unlimited memory locking for GPU
      stack: 67108864             # Stack size for deep learning operations

# Resource monitoring sidecar
  resource-monitor:
    image: prom/node-exporter:latest
    container_name: resource-monitor
    
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    
    ports:
      - "9100:9100"
    
    restart: unless-stopped

# Prometheus monitoring stack
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    
    ports:
      - "9090:9090"
    
    restart: unless-stopped

volumes:
  prometheus_data:
```

### 4. Automated Resource Management

**Intelligent Resource Scaling and Optimization:**

```python
class ResourceManager:
    """Automated resource management and optimization."""
    
    def __init__(self, config: HostResourceConfig):
        self.config = config
        self.ram_monitor = HostRAMMonitor(config)
        self.pcie_monitor = PCIeBandwidthMonitor(config)
        
    async def start_monitoring(self):
        """Start all resource monitoring tasks."""
        
        # Start monitoring tasks concurrently
        tasks = [
            asyncio.create_task(self.ram_monitor.monitor_ram_usage()),
            asyncio.create_task(self.pcie_monitor.monitor_pcie_bandwidth()),
            asyncio.create_task(self._resource_optimization_loop()),
            asyncio.create_task(self._capacity_planning_analysis())
        ]
        
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def _resource_optimization_loop(self):
        """Continuous resource optimization and tuning."""
        
        while True:
            try:
                # Analyze resource usage patterns
                resource_analysis = await self._analyze_resource_patterns()
                
                # Optimize vLLM configurations based on resource usage
                optimizations = await self._generate_optimization_recommendations(resource_analysis)
                
                # Apply safe optimizations automatically
                for optimization in optimizations:
                    if optimization["risk_level"] == "low":
                        await self._apply_optimization(optimization)
                        logging.info(f"Applied optimization: {optimization['description']}")
                
                # Report recommendations for manual review
                manual_optimizations = [opt for opt in optimizations if opt["risk_level"] != "low"]
                if manual_optimizations:
                    await self._report_manual_optimizations(manual_optimizations)
                
            except Exception as e:
                logging.error(f"Resource optimization failed: {e}")
            
            await asyncio.sleep(300)  # Run every 5 minutes
    
    async def _analyze_resource_patterns(self) -> Dict:
        """Analyze resource usage patterns over time."""
        
        # Get current resource metrics
        ram_stats = psutil.virtual_memory()
        
        # Calculate usage patterns
        analysis = {
            "avg_ram_utilization": ram_stats.percent / 100.0,
            "peak_ram_usage": ram_stats.used,
            "swap_space_efficiency": await self._calculate_swap_efficiency(),
            "pcie_utilization_trend": await self._analyze_pcie_trends(),
            "resource_pressure_events": await self._count_pressure_events(),
        }
        
        return analysis
    
    async def _generate_optimization_recommendations(self, analysis: Dict) -> List[Dict]:
        """Generate resource optimization recommendations."""
        
        recommendations = []
        
        # RAM optimization recommendations
        if analysis["avg_ram_utilization"] > 0.85:
            recommendations.append({
                "type": "ram_optimization",
                "description": "Reduce vLLM swap_space allocation due to high RAM usage",
                "action": "reduce_swap_space",
                "current_allocation": self.config.longcontext_pool_swap_gb,
                "recommended_allocation": max(8, self.config.longcontext_pool_swap_gb - 4),
                "risk_level": "medium",
                "expected_impact": "Reduced memory pressure, potential latency trade-off"
            })
        elif analysis["avg_ram_utilization"] < 0.65:
            recommendations.append({
                "type": "ram_optimization", 
                "description": "Increase vLLM swap_space allocation for better performance",
                "action": "increase_swap_space",
                "current_allocation": self.config.longcontext_pool_swap_gb,
                "recommended_allocation": min(24, self.config.longcontext_pool_swap_gb + 4),
                "risk_level": "low",
                "expected_impact": "Improved performance with higher memory availability"
            })
        
        # PCIe optimization recommendations
        if analysis["pcie_utilization_trend"] > 0.8:
            recommendations.append({
                "type": "pcie_optimization",
                "description": "PCIe bandwidth optimization needed",
                "action": "optimize_transfer_patterns",
                "risk_level": "high",
                "expected_impact": "Reduced transfer latency and improved throughput"
            })
        
        return recommendations
    
    async def _apply_optimization(self, optimization: Dict):
        """Apply safe resource optimizations automatically."""
        
        if optimization["action"] == "increase_swap_space" and optimization["risk_level"] == "low":
            # Only apply low-risk optimizations automatically
            new_allocation = optimization["recommended_allocation"]
            
            # This would integrate with vLLM configuration management
            logging.info(f"Would apply swap_space optimization: {new_allocation}GB")
            # await vllm_config_manager.update_swap_space(new_allocation)
```

## Testing Strategy

### Resource Management Testing

```python
# tests/test_host_resource_management.py
import pytest
import asyncio
import psutil
from unittest.mock import Mock, patch
from src.host_resource_manager import HostRAMMonitor, PCIeBandwidthMonitor, HostResourceConfig

class TestHostResourceManagement:
    """Comprehensive testing for host resource management."""
    
    @pytest.fixture
    def resource_config(self):
        return HostResourceConfig(
            throughput_pool_swap_gb=2,
            longcontext_pool_swap_gb=16,
            overhead_buffer_gb=2,
            ram_warning_threshold=0.9,
            ram_critical_threshold=0.95,
            pcie_bandwidth_threshold=10.0
        )
    
    @pytest.fixture
    async def ram_monitor(self, resource_config):
        monitor = HostRAMMonitor(resource_config)
        yield monitor
        # Cleanup any monitoring tasks
        
    async def test_ram_allocation_calculation(self, ram_monitor, resource_config):
        """Test vLLM swap space allocation calculations."""
        
        expected_allocation = (
            resource_config.throughput_pool_swap_gb + 
            resource_config.longcontext_pool_swap_gb + 
            resource_config.overhead_buffer_gb
        ) * 1024 * 1024 * 1024
        
        # Mock RAM statistics
        with patch('psutil.virtual_memory') as mock_ram:
            mock_ram.return_value = Mock(
                total=64 * 1024 * 1024 * 1024,  # 64GB total
                used=30 * 1024 * 1024 * 1024,   # 30GB used
                available=34 * 1024 * 1024 * 1024, # 34GB available  
                percent=46.9  # ~47% utilization
            )
            
            # Run monitoring cycle
            await ram_monitor.monitor_ram_usage()
            
            # Verify allocation calculation
            assert ram_monitor.vllm_swap_allocated._value._value == expected_allocation
    
    async def test_ram_warning_threshold(self, ram_monitor):
        """Test RAM usage warning triggers."""
        
        alerts_triggered = []
        
        async def mock_alert(title, message, severity):
            alerts_triggered.append((title, message, severity))
            
        ram_monitor._send_alert = mock_alert
        
        # Mock high RAM usage
        with patch('psutil.virtual_memory') as mock_ram:
            mock_ram.return_value = Mock(
                total=64 * 1024 * 1024 * 1024,
                used=58 * 1024 * 1024 * 1024,   # 58GB used (90.6%)
                available=6 * 1024 * 1024 * 1024,
                percent=90.6
            )
            
            # Run monitoring cycle  
            await asyncio.sleep(0.1)  # Allow monitoring to run
            
        # Verify warning was triggered
        assert len(alerts_triggered) > 0
        assert "RAM Usage Warning" in alerts_triggered[0][0]
        assert alerts_triggered[0][2] == "warning"
    
    async def test_pcie_bandwidth_monitoring(self, resource_config):
        """Test PCIe bandwidth monitoring and alerting."""
        
        pcie_monitor = PCIeBandwidthMonitor(resource_config)
        alerts_triggered = []
        
        async def mock_alert(title, message, severity):
            alerts_triggered.append((title, message, severity))
            
        pcie_monitor._send_alert = mock_alert
        
        # Mock high PCIe bandwidth usage
        async def mock_high_bandwidth():
            return 12.0  # Above 10 GB/s threshold
            
        pcie_monitor._get_pcie_bandwidth_usage = mock_high_bandwidth
        
        # Run monitoring cycle
        await asyncio.sleep(0.1)
        
        # Verify saturation alert
        assert pcie_monitor.pcie_saturation_events._value._value > 0

class TestContainerResourceLimits:
    """Test container resource limit configurations."""
    
    def test_docker_compose_resource_limits(self):
        """Validate Docker Compose resource configurations."""
        import yaml
        
        # Load Docker Compose configuration
        compose_config = {
            "version": "3.8",
            "services": {
                "ai-job-scraper": {
                    "deploy": {
                        "resources": {
                            "reservations": {
                                "memory": "20G",
                                "cpus": "4.0"
                            },
                            "limits": {
                                "memory": "24G", 
                                "cpus": "6.0"
                            }
                        }
                    }
                }
            }
        }
        
        # Validate memory allocation matches vLLM requirements
        memory_reservation = compose_config["services"]["ai-job-scraper"]["deploy"]["resources"]["reservations"]["memory"]
        memory_limit = compose_config["services"]["ai-job-scraper"]["deploy"]["resources"]["limits"]["memory"]
        
        assert memory_reservation == "20G"  # 18GB swap + 2GB overhead
        assert memory_limit == "24G"        # 4GB overflow buffer
```

## Consequences

### Positive Outcomes

- ✅ **Optimal Resource Utilization**: 18GB swap_space allocation with 20% overhead buffer
- ✅ **Performance Monitoring**: Real-time PCIe bandwidth and RAM utilization tracking
- ✅ **Proactive Alerting**: Early warning system prevents resource exhaustion
- ✅ **Container Optimization**: Proper resource limits with overflow capacity
- ✅ **Capacity Planning**: Trend analysis and resource usage forecasting
- ✅ **Automated Optimization**: Safe resource tuning with manual oversight

### Negative Consequences

- ❌ **Monitoring Overhead**: ~1% CPU utilization for comprehensive monitoring
- ❌ **Memory Requirements**: 64GB+ system RAM recommended for optimal operation
- ❌ **Complexity**: Multi-layer resource management increases operational complexity
- ❌ **Hardware Dependency**: Optimized specifically for RTX 4090 configuration

### Risk Mitigation

1. **Resource Exhaustion Prevention**
   - Multi-threshold alerting (warning at 90%, critical at 95%)
   - Container resource limits with overflow buffers
   - Automatic resource optimization for low-risk scenarios

2. **Monitoring Reliability**
   - Multiple monitoring approaches (psutil, nvidia-ml-py, container stats)
   - Fallback monitoring methods if primary systems fail
   - Health checks and monitoring validation

3. **Performance Impact Minimization**
   - Lightweight monitoring with configurable intervals
   - Asynchronous monitoring to prevent blocking operations
   - Optimized metrics collection and storage

## Implementation Timeline

### Phase 1: Basic Monitoring (Days 1-2)

- [ ] Deploy host RAM monitoring with vLLM swap_space allocation tracking
- [ ] Implement PCIe bandwidth monitoring with basic alerting
- [ ] Setup container resource limits and reservations
- [ ] Configure Prometheus metrics collection

### Phase 2: Advanced Monitoring (Days 3-4)

- [ ] Implement comprehensive alerting system with multiple thresholds
- [ ] Deploy resource trend analysis and capacity planning
- [ ] Setup automated optimization for low-risk scenarios
- [ ] Configure monitoring dashboards and visualization

### Phase 3: Integration and Testing (Days 5-6)

- [ ] Integrate with vLLM two-tier deployment monitoring
- [ ] Test resource exhaustion scenarios and recovery
- [ ] Validate container limits under high load
- [ ] Performance testing with resource constraints

### Phase 4: Production Deployment (Days 7-8)

- [ ] Deploy monitoring stack to production environment
- [ ] Configure production alerting and escalation procedures
- [ ] Setup capacity planning and scaling recommendations
- [ ] Documentation and operational runbook completion

## Success Metrics

| Metric | Target | Validation Method |
|--------|---------|------------------|
| Host RAM Utilization | <90% sustained | Continuous monitoring |
| PCIe Bandwidth | <10GB/s sustained | Real-time bandwidth tracking |
| Container Memory Limits | 20GB reservation + 4GB overflow | Docker stats validation |
| Monitoring Overhead | <1% CPU utilization | Performance impact measurement |
| Alert Response Time | <30 seconds | Alert system testing |
| Resource Optimization | 10% efficiency improvement | Before/after comparison |

## Changelog

### v1.0 - August 19, 2025

- Comprehensive host system resource management for vLLM deployment
- Host RAM monitoring with swap_space allocation tracking  
- PCIe bandwidth monitoring with saturation alerting
- Container resource limits with overflow capacity
- Automated resource optimization with manual oversight
- Complete monitoring and alerting infrastructure

---

*This ADR provides essential host system resource management for optimal vLLM two-tier deployment performance, preventing resource exhaustion and enabling 2-3x performance improvement validated through expert research.*
