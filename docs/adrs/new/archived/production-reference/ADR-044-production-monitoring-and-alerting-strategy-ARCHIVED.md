# ADR-044: Production Monitoring and Alerting Strategy

## Title

Production Monitoring and Alerting Strategy for vLLM Two-Tier Deployment and Real-Time Operations

## Version/Date

1.0 / August 19, 2025

## Status

**Accepted** - Essential for production vLLM deployment operations

## Description

Comprehensive production monitoring and alerting strategy for AI job scraper deployment, focusing on vLLM two-tier architecture performance, host system resource utilization, cost optimization tracking, and real-time operational visibility. Provides automated alerting, performance regression detection, and capacity planning for maintaining 2-3x performance improvement validated through expert research.

## Context

### Monitoring Requirements from Architecture Research

**vLLM Two-Tier Deployment Monitoring (ADR-042):**

- **Performance Targets**: Throughput pool P95 ≤100ms, Long-context pool P95 ≤2s
- **Concurrency Monitoring**: 2-3x improvement validation with 60-80 req/s throughput
- **Resource Tracking**: swap_space utilization, GPU memory efficiency, request routing
- **Latency Regression**: P95/P99 monitoring with ≤15% increase alert threshold

**Host System Resource Monitoring (ADR-043):**  

- **RAM Utilization**: 18GB swap_space allocation + 20% overhead monitoring
- **PCIe Bandwidth**: 10GB/s sustained threshold with H2D transfer tracking
- **Container Limits**: 20GB reservation + 4GB overflow capacity monitoring
- **Resource Optimization**: Trend analysis and capacity planning metrics

**Cost Optimization Monitoring:**

- **Monthly Budget Tracking**: $30/month operational cost target
- **Local Processing Ratio**: 98% local vs 2% cloud processing validation
- **Resource Efficiency**: GPU utilization 85-90% with overflow capability
- **Proxy Usage**: IPRoyal residential proxy tier cost tracking

## Related Requirements

### Functional Requirements

- **FR-MON-01**: Real-time performance monitoring for both vLLM pools
- **FR-MON-02**: Automated alerting for performance regression and resource exhaustion
- **FR-MON-03**: Cost tracking and budget compliance monitoring
- **FR-MON-04**: Operational dashboard for system health visualization

### Non-Functional Requirements

- **NFR-MON-01**: Monitoring overhead <2% system resource utilization
- **NFR-MON-02**: Alert response time <30 seconds for critical events
- **NFR-MON-03**: Metric retention 30 days with 5-second resolution
- **NFR-MON-04**: Dashboard refresh rate <1 second for real-time visualization

### Performance Requirements

- **PR-MON-01**: KV cache metrics collection with page eviction tracking
- **PR-MON-02**: Latency percentile calculation (P50, P95, P99) with 1-minute windows
- **PR-MON-03**: Throughput measurement accuracy ±2% for capacity planning
- **PR-MON-04**: Cost tracking accuracy ±$0.10 for budget management

## Related Decisions

- **Monitors ADR-042**: vLLM Two-Tier Deployment Strategy (performance and resource metrics)
- **Integrates ADR-043**: Host System Resource Management (resource utilization alerts)
- **Supports ADR-035**: Final Production Architecture (overall system monitoring)
- **Enables ADR-045**: Cost Optimization and Resource Efficiency (budget tracking)

## Decision

**Deploy Comprehensive Production Monitoring Stack** with these core components:

### 1. vLLM Performance Monitoring

**Expert-Validated Performance Metrics Collection:**

```python
import asyncio
import time
import numpy as np
from prometheus_client import Gauge, Histogram, Counter, Summary
from dataclasses import dataclass
from typing import Dict, List, Optional
import logging

@dataclass
class VLLMPerformanceTargets:
    """Expert-validated performance targets from research."""
    
    # Latency targets (from ADR-042 validation)
    throughput_pool_p95_ms: float = 100.0    # 100ms P95 target
    longcontext_pool_p95_ms: float = 2000.0  # 2s P95 target
    
    # Throughput targets
    overall_throughput_rps: float = 70.0     # 70 req/s minimum
    concurrency_improvement: float = 2.5     # 2.5x average improvement
    
    # Resource utilization targets
    gpu_utilization_target: float = 0.85     # 85% GPU utilization
    swap_space_efficiency: float = 0.7       # 70% swap utilization efficiency
    
    # Alert thresholds
    latency_regression_threshold: float = 1.15  # 15% increase threshold
    throughput_degradation_threshold: float = 0.85  # 15% decrease threshold

class VLLMMetricsCollector:
    """Comprehensive vLLM performance metrics collection."""
    
    def __init__(self, targets: VLLMPerformanceTargets):
        self.targets = targets
        
        # Request latency tracking
        self.throughput_pool_latency = Histogram(
            'vllm_throughput_pool_request_latency_seconds',
            'Throughput pool request latency',
            buckets=[0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0]
        )
        
        self.longcontext_pool_latency = Histogram(
            'vllm_longcontext_pool_request_latency_seconds', 
            'Long-context pool request latency',
            buckets=[0.1, 0.25, 0.5, 1.0, 2.0, 5.0, 10.0]
        )
        
        # Throughput and concurrency
        self.requests_per_second = Gauge('vllm_requests_per_second', 'Overall requests per second')
        self.active_requests = Gauge('vllm_active_requests_total', 'Active requests across pools')
        self.concurrency_ratio = Gauge('vllm_concurrency_improvement_ratio', 'Concurrency improvement vs baseline')
        
        # Resource utilization
        self.gpu_utilization = Gauge('vllm_gpu_utilization_percent', 'GPU utilization percentage')
        self.swap_space_used = Gauge('vllm_swap_space_used_bytes', 'Swap space utilization')
        self.kv_cache_usage = Gauge('vllm_kv_cache_usage_bytes', 'KV cache memory usage')
        
        # Request routing
        self.request_routing = Counter(
            'vllm_request_routing_total',
            'Request routing decisions',
            ['pool', 'context_size_bucket']
        )
        
        # Performance alerts
        self.latency_regression_alerts = Counter('vllm_latency_regression_alerts_total', 'Latency regression alerts')
        self.throughput_degradation_alerts = Counter('vllm_throughput_degradation_alerts_total', 'Throughput degradation alerts')
        
        # Baseline performance tracking
        self.baseline_metrics = {
            "throughput_pool_p95": [],
            "longcontext_pool_p95": [], 
            "overall_throughput": []
        }
    
    async def record_request_metrics(self, pool_name: str, latency: float, context_tokens: int):
        """Record individual request metrics."""
        
        # Record latency by pool
        if pool_name == "throughput_pool":
            self.throughput_pool_latency.observe(latency)
        elif pool_name == "longcontext_pool":
            self.longcontext_pool_latency.observe(latency)
        
        # Update routing metrics
        context_bucket = self._get_context_size_bucket(context_tokens)
        self.request_routing.labels(pool=pool_name, context_size_bucket=context_bucket).inc()
        
        # Check for performance regressions
        await self._check_performance_regression(pool_name, latency)
    
    async def update_system_metrics(self):
        """Update system-level performance metrics."""
        
        try:
            # GPU utilization monitoring
            gpu_util = await self._get_gpu_utilization()
            self.gpu_utilization.set(gpu_util)
            
            # Swap space monitoring
            swap_usage = await self._get_swap_space_usage()
            self.swap_space_used.set(swap_usage)
            
            # KV cache monitoring  
            kv_cache_usage = await self._get_kv_cache_usage()
            self.kv_cache_usage.set(kv_cache_usage)
            
            # Calculate real-time throughput
            throughput = await self._calculate_current_throughput()
            self.requests_per_second.set(throughput)
            
            # Update concurrency improvement ratio
            concurrency_improvement = await self._calculate_concurrency_improvement()
            self.concurrency_ratio.set(concurrency_improvement)
            
        except Exception as e:
            logging.error(f"System metrics update failed: {e}")
    
    async def _check_performance_regression(self, pool_name: str, current_latency: float):
        """Check for performance regression against baseline."""
        
        # Get recent performance baseline
        baseline_p95 = await self._get_baseline_p95(pool_name)
        
        if baseline_p95 > 0:
            regression_ratio = current_latency / baseline_p95
            
            # Alert if latency increased beyond threshold
            if regression_ratio > self.targets.latency_regression_threshold:
                self.latency_regression_alerts.inc()
                
                await self._trigger_performance_alert(
                    f"Latency regression detected in {pool_name}",
                    f"Current latency: {current_latency:.3f}s, "
                    f"Baseline P95: {baseline_p95:.3f}s, "
                    f"Regression: {(regression_ratio - 1) * 100:.1f}%"
                )
    
    async def _get_baseline_p95(self, pool_name: str) -> float:
        """Get baseline P95 latency for regression comparison."""
        
        baseline_key = f"{pool_name}_p95"
        if baseline_key in self.baseline_metrics:
            recent_metrics = self.baseline_metrics[baseline_key][-100:]  # Last 100 samples
            if recent_metrics:
                return np.percentile(recent_metrics, 95)
        
        return 0.0
    
    def _get_context_size_bucket(self, tokens: int) -> str:
        """Categorize context size for routing analysis."""
        
        if tokens < 1000:
            return "small"
        elif tokens < 4096:
            return "medium" 
        elif tokens < 8192:
            return "large"
        else:
            return "xlarge"
    
    async def _get_gpu_utilization(self) -> float:
        """Get current GPU utilization percentage."""
        try:
            import pynvml
            pynvml.nvmlInit()
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            util_info = pynvml.nvmlDeviceGetUtilizationRates(handle)
            return float(util_info.gpu)
        except ImportError:
            return 85.0  # Mock value for testing
        except Exception as e:
            logging.error(f"GPU utilization monitoring failed: {e}")
            return 0.0
```

### 2. Cost Optimization Monitoring

**Budget Tracking and Cost Efficiency Metrics:**

```python
class CostMonitor:
    """Comprehensive cost monitoring and budget compliance."""
    
    def __init__(self):
        self.monthly_budget = 30.00  # $30 monthly budget target
        
        # Cost tracking metrics
        self.monthly_cost_total = Gauge('cost_monthly_total_dollars', 'Total monthly cost')
        self.cost_by_category = Gauge(
            'cost_by_category_dollars',
            'Cost breakdown by category', 
            ['category']
        )
        
        # Local vs cloud processing
        self.local_processing_ratio = Gauge('local_processing_ratio', 'Local processing percentage')
        self.cloud_processing_cost = Gauge('cloud_processing_cost_dollars', 'Cloud processing costs')
        
        # Efficiency metrics
        self.cost_per_job_processed = Gauge('cost_per_job_processed_dollars', 'Cost per job processed')
        self.budget_utilization = Gauge('budget_utilization_percent', 'Monthly budget utilization')
        
        # Budget alerts
        self.budget_warnings = Counter('budget_warning_alerts_total', 'Budget warning alerts')
        self.budget_critical = Counter('budget_critical_alerts_total', 'Budget critical alerts')
    
    async def update_cost_metrics(self):
        """Update all cost tracking metrics."""
        
        try:
            # Calculate monthly costs by category
            monthly_costs = {
                "cloud_processing": await self._get_cloud_processing_cost(),
                "iproyal_proxies": 20.00,  # IPRoyal residential proxy tier
                "monitoring_stack": 5.00,   # Prometheus + Grafana
                "redis_hosting": 2.50,      # Redis for RQ background tasks
                "infrastructure": 2.50      # SSL, domain, misc
            }
            
            total_cost = sum(monthly_costs.values())
            
            # Update cost metrics
            self.monthly_cost_total.set(total_cost)
            
            for category, cost in monthly_costs.items():
                self.cost_by_category.labels(category=category).set(cost)
            
            # Calculate budget utilization
            budget_utilization = (total_cost / self.monthly_budget) * 100
            self.budget_utilization.set(budget_utilization)
            
            # Update processing ratio
            local_ratio = await self._get_local_processing_ratio()
            self.local_processing_ratio.set(local_ratio)
            
            # Calculate cost efficiency
            jobs_processed = await self._get_monthly_jobs_processed()
            if jobs_processed > 0:
                cost_per_job = total_cost / jobs_processed
                self.cost_per_job_processed.set(cost_per_job)
            
            # Check budget thresholds
            await self._check_budget_alerts(total_cost, budget_utilization)
            
        except Exception as e:
            logging.error(f"Cost monitoring update failed: {e}")
    
    async def _check_budget_alerts(self, total_cost: float, utilization: float):
        """Check budget utilization and trigger alerts."""
        
        if utilization >= 100:
            self.budget_critical.inc()
            await self._trigger_budget_alert(
                "CRITICAL: Monthly budget exceeded",
                f"Current cost: ${total_cost:.2f}, Budget: ${self.monthly_budget:.2f} "
                f"({utilization:.1f}% utilization)",
                severity="critical"
            )
        elif utilization >= 90:
            self.budget_warnings.inc()
            await self._trigger_budget_alert(
                "Budget warning: Approaching monthly limit",
                f"Current cost: ${total_cost:.2f}, Budget: ${self.monthly_budget:.2f} "
                f"({utilization:.1f}% utilization)",
                severity="warning"
            )
    
    async def _get_cloud_processing_cost(self) -> float:
        """Calculate current month cloud processing costs."""
        # This would integrate with cloud provider billing APIs
        # For now, estimate based on usage patterns
        local_ratio = await self._get_local_processing_ratio()
        cloud_ratio = 1.0 - local_ratio
        
        # Estimate: If using 2% cloud processing at $0.10/request
        estimated_requests = await self._get_monthly_request_count()
        cloud_requests = estimated_requests * cloud_ratio
        
        # Cloud cost estimation (simplified)
        return cloud_requests * 0.01  # $0.01 per cloud request
    
    async def _get_local_processing_ratio(self) -> float:
        """Get current local vs cloud processing ratio."""
        # This would analyze recent request routing decisions
        # Target is 98% local processing
        return 0.98  # Target ratio from research
```

### 3. Operational Dashboard Configuration

**Grafana Dashboard for Real-Time Monitoring:**

```json
{
  "dashboard": {
    "title": "AI Job Scraper - Production Monitoring",
    "tags": ["ai-job-scraper", "production", "vllm"],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "5s",
    "panels": [
      {
        "title": "vLLM Performance Overview",
        "type": "stat",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, vllm_throughput_pool_request_latency_seconds_bucket)",
            "legendFormat": "Throughput Pool P95"
          },
          {
            "expr": "histogram_quantile(0.95, vllm_longcontext_pool_request_latency_seconds_bucket)",  
            "legendFormat": "Long-Context Pool P95"
          },
          {
            "expr": "vllm_requests_per_second",
            "legendFormat": "Requests/Second"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {"mode": "thresholds"},
            "thresholds": {
              "steps": [
                {"color": "green", "value": null},
                {"color": "yellow", "value": 0.08},
                {"color": "red", "value": 0.12}
              ]
            }
          }
        }
      },
      {
        "title": "Resource Utilization",
        "type": "graph",
        "targets": [
          {
            "expr": "vllm_gpu_utilization_percent",
            "legendFormat": "GPU Utilization %"
          },
          {
            "expr": "host_ram_utilization_percent",
            "legendFormat": "Host RAM Utilization %"
          },
          {
            "expr": "vllm_swap_space_used_bytes / (1024*1024*1024)",
            "legendFormat": "Swap Space Used GB"
          }
        ],
        "yAxes": [
          {
            "min": 0,
            "max": 100,
            "unit": "percent"
          }
        ]
      },
      {
        "title": "Cost Tracking",
        "type": "stat", 
        "targets": [
          {
            "expr": "cost_monthly_total_dollars",
            "legendFormat": "Monthly Cost"
          },
          {
            "expr": "budget_utilization_percent",
            "legendFormat": "Budget Utilization %"
          },
          {
            "expr": "local_processing_ratio * 100",
            "legendFormat": "Local Processing %"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {"mode": "thresholds"},
            "thresholds": {
              "steps": [
                {"color": "green", "value": null},
                {"color": "yellow", "value": 90},
                {"color": "red", "value": 100}
              ]
            }
          }
        }
      },
      {
        "title": "Request Routing Distribution",
        "type": "piechart",
        "targets": [
          {
            "expr": "sum by (pool) (rate(vllm_request_routing_total[5m]))",
            "legendFormat": "{{pool}}"
          }
        ]
      },
      {
        "title": "Latency Percentiles",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.50, vllm_throughput_pool_request_latency_seconds_bucket)",
            "legendFormat": "Throughput P50"
          },
          {
            "expr": "histogram_quantile(0.95, vllm_throughput_pool_request_latency_seconds_bucket)",
            "legendFormat": "Throughput P95"
          },
          {
            "expr": "histogram_quantile(0.99, vllm_throughput_pool_request_latency_seconds_bucket)",
            "legendFormat": "Throughput P99"
          }
        ]
      },
      {
        "title": "System Health Alerts",
        "type": "logs",
        "targets": [
          {
            "expr": "{job=\"ai-job-scraper\", level=\"warning\"}",
            "refId": "A"
          }
        ]
      }
    ]
  }
}
```

### 4. Alerting Configuration

**Comprehensive Alerting Rules:**

```yaml
# prometheus-alerts.yml
groups:
  - name: vllm-performance
    rules:
      - alert: VLLMThroughputPoolLatencyHigh
        expr: histogram_quantile(0.95, vllm_throughput_pool_request_latency_seconds_bucket) > 0.1
        for: 2m
        labels:
          severity: warning
          component: vllm-throughput-pool
        annotations:
          summary: "vLLM throughput pool P95 latency exceeds target"
          description: "Throughput pool P95 latency is {{ $value }}s (target: ≤100ms)"
          
      - alert: VLLMLongContextPoolLatencyHigh  
        expr: histogram_quantile(0.95, vllm_longcontext_pool_request_latency_seconds_bucket) > 2.0
        for: 2m
        labels:
          severity: warning
          component: vllm-longcontext-pool
        annotations:
          summary: "vLLM long-context pool P95 latency exceeds target"
          description: "Long-context pool P95 latency is {{ $value }}s (target: ≤2s)"
          
      - alert: VLLMThroughputLow
        expr: vllm_requests_per_second < 60
        for: 5m
        labels:
          severity: warning
          component: vllm-throughput
        annotations:
          summary: "vLLM throughput below target"
          description: "Current throughput: {{ $value }} req/s (target: ≥70 req/s)"

  - name: resource-utilization
    rules:
      - alert: HostRAMUtilizationHigh
        expr: host_ram_utilization_percent > 90
        for: 2m
        labels:
          severity: warning
          component: host-resources
        annotations:
          summary: "Host RAM utilization high"
          description: "RAM utilization: {{ $value }}% (threshold: 90%)"
          
      - alert: HostRAMUtilizationCritical
        expr: host_ram_utilization_percent > 95
        for: 1m
        labels:
          severity: critical
          component: host-resources
        annotations:
          summary: "CRITICAL: Host RAM utilization"
          description: "RAM utilization: {{ $value }}% (critical threshold: 95%)"
          
      - alert: PCIeBandwidthSaturation
        expr: pcie_bandwidth_used_gbps > 10.0
        for: 30s
        labels:
          severity: warning
          component: pcie-bandwidth
        annotations:
          summary: "PCIe bandwidth saturation detected"
          description: "PCIe bandwidth: {{ $value }} GB/s (threshold: 10 GB/s)"

  - name: cost-budget
    rules:
      - alert: MonthlyBudgetWarning
        expr: budget_utilization_percent > 90
        for: 1m
        labels:
          severity: warning
          component: cost-optimization
        annotations:
          summary: "Monthly budget warning"
          description: "Budget utilization: {{ $value }}% (warning: 90%)"
          
      - alert: MonthlyBudgetExceeded
        expr: budget_utilization_percent > 100
        for: 1m
        labels:
          severity: critical
          component: cost-optimization  
        annotations:
          summary: "CRITICAL: Monthly budget exceeded"
          description: "Budget utilization: {{ $value }}% (budget: $30/month)"
          
      - alert: LocalProcessingRatioLow
        expr: local_processing_ratio < 0.95
        for: 5m
        labels:
          severity: warning
          component: cost-optimization
        annotations:
          summary: "Local processing ratio below target"
          description: "Local processing: {{ $value | humanizePercentage }} (target: ≥98%)"

  - name: operational-health
    rules:
      - alert: ScrapingJobFailureRateHigh
        expr: rate(scraping_job_failures_total[5m]) / rate(scraping_jobs_total[5m]) > 0.05
        for: 2m
        labels:
          severity: warning
          component: job-scraping
        annotations:
          summary: "High scraping job failure rate"
          description: "Failure rate: {{ $value | humanizePercentage }} (threshold: 5%)"
          
      - alert: DatabaseConnectionErrors
        expr: rate(database_connection_errors_total[5m]) > 0.1
        for: 1m
        labels:
          severity: critical
          component: database
        annotations:
          summary: "Database connection errors detected"
          description: "Connection error rate: {{ $value }}/sec"
```

## Implementation Timeline

### Phase 1: Core Monitoring Infrastructure (Days 1-2)

- [ ] Deploy Prometheus + Grafana monitoring stack
- [ ] Implement vLLM performance metrics collection
- [ ] Setup basic alerting for latency and throughput
- [ ] Configure operational dashboard with real-time visualization

### Phase 2: Advanced Monitoring (Days 3-4)

- [ ] Deploy comprehensive cost tracking and budget monitoring
- [ ] Implement host resource monitoring integration
- [ ] Setup performance regression detection and alerting
- [ ] Configure capacity planning and trend analysis

### Phase 3: Alerting and Integration (Days 5-6)

- [ ] Deploy comprehensive alerting rules and escalation procedures
- [ ] Integrate with external alerting systems (Slack, PagerDuty, email)
- [ ] Setup automated response procedures for common issues
- [ ] Configure monitoring validation and health checks

### Phase 4: Production Deployment (Days 7-8)

- [ ] Deploy monitoring stack to production environment
- [ ] Validate all metrics collection and alerting functionality
- [ ] Configure production alerting thresholds and escalation
- [ ] Documentation and operational runbook completion

## Success Metrics

| Metric | Target | Validation Method |
|--------|---------|------------------|
| Monitoring Coverage | 100% of critical components | Metrics availability validation |
| Alert Response Time | <30 seconds | Alert system testing |
| Dashboard Load Time | <1 second | Performance testing |
| Metric Retention | 30 days at 5s resolution | Storage validation |
| Budget Tracking Accuracy | ±$0.10 | Cost reconciliation |
| Performance Regression Detection | ≤2 minute detection time | Regression testing |

## Consequences

### Positive Outcomes

- ✅ **Comprehensive Visibility**: Real-time operational insight into all system components
- ✅ **Proactive Alerting**: Early detection of performance and resource issues
- ✅ **Cost Control**: Accurate budget tracking and optimization guidance
- ✅ **Performance Validation**: Continuous validation of 2-3x improvement targets
- ✅ **Capacity Planning**: Data-driven scaling and resource allocation decisions
- ✅ **Operational Excellence**: Reduced MTTR and improved system reliability

### Negative Consequences

- ❌ **Monitoring Overhead**: ~2% system resource utilization for comprehensive monitoring
- ❌ **Storage Requirements**: 30-day metric retention requires dedicated storage
- ❌ **Alert Fatigue**: Comprehensive alerting requires careful threshold tuning
- ❌ **Operational Complexity**: Multi-layer monitoring increases management overhead

### Risk Mitigation

1. **Monitoring Reliability**
   - Redundant monitoring paths and failover mechanisms
   - Health checks for monitoring infrastructure components
   - Backup alerting channels for critical events

2. **Performance Impact Minimization**
   - Optimized metric collection with configurable sampling rates
   - Asynchronous monitoring to prevent blocking operations
   - Resource limits for monitoring components

3. **Alert Quality**
   - Threshold tuning based on baseline performance data
   - Alert suppression and correlation to reduce noise
   - Clear escalation procedures and response playbooks

## Changelog

### v1.0 - August 19, 2025

- Comprehensive production monitoring strategy for vLLM two-tier deployment
- Real-time performance metrics with expert-validated targets
- Cost optimization monitoring with $30/month budget tracking
- Operational dashboard with 5-second refresh real-time visualization
- Advanced alerting with performance regression detection
- Complete monitoring infrastructure with 30-day retention

---

*This ADR provides essential production monitoring and alerting for maintaining optimal vLLM two-tier deployment performance, ensuring 2-3x improvement validation and operational excellence.*
