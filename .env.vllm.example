# vLLM Server Environment Configuration
# Copy this file to .env and configure your settings

# =============================================================================
# vLLM Server Configuration
# =============================================================================

# API Key for vLLM server authentication
# Generate a secure key: openssl rand -hex 32
VLLM_API_KEY=vllm-server-your-secure-key-here

# Server endpoints (usually don't need to change)
VLLM_BASE_URL=http://localhost:8000
VLLM_HEALTH_URL=http://localhost:8000/health
VLLM_METRICS_URL=http://localhost:8001/metrics

# =============================================================================
# GPU Configuration
# =============================================================================

# CUDA device to use (0 for first GPU)
CUDA_VISIBLE_DEVICES=0

# GPU memory utilization (0.8-0.9 recommended for RTX 4090)
VLLM_GPU_MEMORY_UTILIZATION=0.9

# Swap space for CPU offloading (4-8 GB recommended)
VLLM_SWAP_SPACE=4

# =============================================================================
# Model Configuration
# =============================================================================

# Model name (don't change unless using different model)
VLLM_MODEL=Qwen/Qwen3-4B-Instruct-2507-FP8

# Model quantization method
VLLM_QUANTIZATION=fp8

# Maximum context length (tokens)
VLLM_MAX_MODEL_LEN=8192

# Maximum concurrent sequences
VLLM_MAX_NUM_SEQS=128

# Trust remote code (required for Qwen models)
VLLM_TRUST_REMOTE_CODE=true

# =============================================================================
# Performance Tuning
# =============================================================================

# Enable prefix caching for better performance
VLLM_ENABLE_PREFIX_CACHING=true

# Enable chunked prefill
VLLM_ENABLE_CHUNKED_PREFILL=true

# Maximum batched tokens
VLLM_MAX_NUM_BATCHED_TOKENS=2048

# Scheduler delay factor (0.0 for immediate scheduling)
VLLM_SCHEDULER_DELAY_FACTOR=0.0

# Disable request logging for performance
VLLM_DISABLE_LOG_REQUESTS=true

# =============================================================================
# Monitoring Configuration
# =============================================================================

# Enable metrics endpoint
VLLM_ENABLE_METRICS=true

# Metrics port
VLLM_METRICS_PORT=8001

# Log level (DEBUG, INFO, WARNING, ERROR)
VLLM_LOG_LEVEL=INFO

# Prometheus monitoring (optional)
PROMETHEUS_ENABLED=false
PROMETHEUS_PORT=9090

# =============================================================================
# Docker Configuration
# =============================================================================

# Container name
VLLM_CONTAINER_NAME=ai-job-scraper-vllm

# Host port for vLLM server
VLLM_HOST_PORT=8000

# Host port for metrics
VLLM_METRICS_HOST_PORT=8001

# Docker network name
VLLM_NETWORK_NAME=vllm-network

# =============================================================================
# Resource Limits
# =============================================================================

# Container memory limit (20GB recommended for RTX 4090)
VLLM_MEMORY_LIMIT=20g

# Container CPU limit (number of cores)
VLLM_CPU_LIMIT=8

# =============================================================================
# Storage Configuration
# =============================================================================

# Hugging Face cache directory
HF_CACHE_DIR=${HOME}/.cache/huggingface

# vLLM logs directory
VLLM_LOGS_DIR=./logs/vllm

# Configuration directory
VLLM_CONFIG_DIR=./config/vllm

# =============================================================================
# Development Settings
# =============================================================================

# Enable development mode (more verbose logging)
VLLM_DEV_MODE=false

# Enable debug mode
VLLM_DEBUG=false

# Validation timeout (seconds)
VLLM_VALIDATION_TIMEOUT=60

# Health check interval (seconds)
VLLM_HEALTH_CHECK_INTERVAL=30

# =============================================================================
# Integration Settings
# =============================================================================

# LiteLLM integration
LITELLM_CONFIG_PATH=config/litellm.yaml

# OpenAI API compatibility
OPENAI_API_KEY=your-openai-api-key-here

# =============================================================================
# Advanced Configuration (rarely need to change)
# =============================================================================

# KV cache data type (fp8 for memory efficiency)
VLLM_KV_CACHE_DTYPE=fp8

# Tensor parallel size (1 for single GPU)
VLLM_TENSOR_PARALLEL_SIZE=1

# Pipeline parallel size
VLLM_PIPELINE_PARALLEL_SIZE=1

# Worker use ray (false for single node)
VLLM_WORKER_USE_RAY=false

# Engine use ray (false for single node)
VLLM_ENGINE_USE_RAY=false

# =============================================================================
# Backup and Recovery
# =============================================================================

# Backup directory for configurations
VLLM_BACKUP_DIR=./backups/vllm

# Model backup enabled
VLLM_MODEL_BACKUP_ENABLED=false

# =============================================================================
# Security Settings
# =============================================================================

# Enable CORS (set to true if accessing from browser)
VLLM_ENABLE_CORS=true

# Allowed origins (comma-separated)
VLLM_ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8501

# Enable SSL/TLS (for production)
VLLM_ENABLE_SSL=false

# SSL certificate paths (if enabled)
VLLM_SSL_CERT_PATH=
VLLM_SSL_KEY_PATH=

# =============================================================================
# Usage Notes
# =============================================================================

# 1. Copy this file to .env: cp .env.vllm.example .env
# 2. Generate a secure API key: openssl rand -hex 32
# 3. Adjust GPU memory utilization based on available VRAM
# 4. Increase swap space if handling large requests
# 5. Enable monitoring for production deployments
# 6. Set resource limits appropriate for your hardware
# 7. Configure backup settings for production use

# For more configuration options, see:
# - vLLM documentation: https://docs.vllm.ai/
# - Docker Compose file: docker-compose.vllm.yml
# - Deployment guide: docs/VLLM_DEPLOYMENT.md