# vLLM Server Configuration
# Advanced settings for vLLM local deployment
# Used by docker-compose.vllm.yml and vLLM scripts

# ===================================================================
# vLLM Model Configuration
# ===================================================================

# Model to load (Qwen3-4B with FP8 quantization for RTX 4090)
VLLM_MODEL=Qwen/Qwen3-4B-Instruct-2507-FP8

# Quantization method for memory efficiency
VLLM_QUANTIZATION=fp8

# Maximum context length for the model
VLLM_MAX_MODEL_LEN=8192

# ===================================================================
# vLLM Performance Settings
# ===================================================================

# GPU memory utilization (0.9 = 90% of 24GB = 21.6GB)
VLLM_GPU_MEMORY_UTILIZATION=0.9

# CPU memory swap space for overflow (in GB)
VLLM_SWAP_SPACE=4

# Maximum number of concurrent sequences
VLLM_MAX_NUM_SEQS=128

# Enable prefix caching for performance
VLLM_ENABLE_PREFIX_CACHING=true

# ===================================================================
# vLLM Server Configuration
# ===================================================================

# Server port (must match VLLM_BASE_URL port in .env)
VLLM_PORT=8000

# Server host binding (localhost for security)
VLLM_HOST=0.0.0.0

# Worker process configuration
VLLM_WORKER_USE_RAY=false
VLLM_TENSOR_PARALLEL_SIZE=1

# ===================================================================
# Advanced vLLM Settings
# ===================================================================

# Token sampling settings
VLLM_SEED=42
VLLM_MAX_LOGPROBS=5

# Engine arguments
VLLM_DISABLE_LOG_STATS=false
VLLM_DISABLE_LOG_REQUESTS=false

# Trust remote code (required for some models)
VLLM_TRUST_REMOTE_CODE=true